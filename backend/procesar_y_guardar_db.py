import os
import requests
from dotenv import load_dotenv
import google.generativeai as genai
from time import sleep
from tqdm import tqdm
from datetime import datetime
import hashlib
import re
import db 
from bs4 import BeautifulSoup
import trafilatura

load_dotenv()
GNEWS_API_KEY = os.getenv("GNEWS_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

ES_PRODUCCION = os.getenv("ENVIRONMENT") == "production"

if not GNEWS_API_KEY or not GEMINI_API_KEY:
    raise ValueError("‚ö†Ô∏è Aseg√∫rate de configurar GNEWS_API_KEY y GEMINI_API_KEY en el archivo .env")

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel("gemini-2.5-flash")

CATEGORIAS = {
    "business": "Negocios", "entertainment": "Entretenimiento", "health": "Salud",
    "science": "Ciencia", "sports": "Deportes", "technology": "Tecnolog√≠a", "general": "General"
}

MAX_NOTICIAS_POR_CATEGORIA = 2
TIEMPO_ESPERA_ENTRE_REQUESTS = 2
MAX_PALABRAS_RESUMEN = 350
MAX_PALABRAS_SCRAPING = 600
MIN_PALABRAS_CONTENIDO_VALIDO = 50
MIN_PALABRAS_RESUMEN_SIGNIFICATIVO = 80

def generar_hash_titulo(titulo):
    return hashlib.md5(titulo.strip().lower().encode('utf-8')).hexdigest()



def obtener_noticias_por_categoria(categoria, max_noticias=MAX_NOTICIAS_POR_CATEGORIA, urls_existentes=None):
    if urls_existentes is None:
        urls_existentes = set()
    
    print(f"üì° Buscando {max_noticias} noticias NUEVAS de: '{CATEGORIAS.get(categoria, categoria)}'...")
    
    noticias_nuevas = []
    urls_encontradas = set()
    
    try:
        url = (f"https://gnews.io/api/v4/top-headlines?"
               f"category={categoria}&lang=es&max={max_noticias * 3}&apikey={GNEWS_API_KEY}")
        
        sleep(TIEMPO_ESPERA_ENTRE_REQUESTS)
        
        resp = requests.get(url, timeout=15)
        
        if resp.status_code == 429:
            print(f"‚è≥ Rate limit alcanzado para {categoria}, esperando...")
            sleep(15)
            return []
            
        if resp.status_code != 200:
            print(f"‚ùå Error HTTP {resp.status_code} para {categoria}")
            return []
            
        data = resp.json()
        articulos = data.get("articles", [])
       
        for articulo in articulos:
            if not all([articulo.get("url"), articulo.get("title"), articulo.get("description")]):
                continue
                
            url_noticia = articulo.get("url")
            

            if (url_noticia not in urls_existentes and 
                url_noticia not in urls_encontradas and
                len(articulo.get("title", "").strip()) > 10 and
                len(articulo.get("description", "").strip()) > 50):
                
                if not db.noticia_existe(articulo.get("title"), url_noticia):
                    articulo['categoria_asignada'] = CATEGORIAS.get(categoria, "General")
                    noticias_nuevas.append(articulo)
                    urls_encontradas.add(url_noticia)
                    
                    if len(noticias_nuevas) >= max_noticias:
                        break
        
    except requests.exceptions.Timeout:
        print(f"‚è∞ Timeout al obtener noticias de {categoria}")
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error de conexi√≥n con GNews para '{categoria}': {e}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error inesperado en {categoria}: {e}")
    
    print(f"‚úÖ Encontradas {len(noticias_nuevas)} noticias v√°lidas para '{CATEGORIAS.get(categoria, categoria)}'")
    return noticias_nuevas

def scrapear_texto_robusto(url, fallback_description=None):
    """Scraping robusto con m√∫ltiples m√©todos de extracci√≥n"""
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    

    try:
        downloaded = trafilatura.fetch_url(url, headers=headers)
        if downloaded:
            content = trafilatura.extract(
                downloaded, 
                include_comments=False, 
                include_tables=False,
                no_fallback=True
            )
            if content and len(content.split()) > 100:
                print(f"‚úÖ Trafilatura: {len(content.split())} palabras")

                return ' '.join(content.split()[:MAX_PALABRAS_SCRAPING])
    except Exception as e:
        print(f"‚ö†Ô∏è Trafilatura fall√≥: {e}")


    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        selectores = [
            'article .article-content', 'article .story-content', '.news-content',
            '.entry-content', '.post-content', '[class*="content"]',
            'article p', '.article-body', '.news-body'
        ]
        
        for selector in selectores:
            elements = soup.select(selector)
            if elements:
                text_content = ' '.join([elem.get_text(strip=True) for elem in elements])
                if len(text_content.split()) > 100:
                    print(f"‚úÖ BeautifulSoup con selector '{selector}': {len(text_content.split())} palabras")
                    return ' '.join(text_content.split()[:MAX_PALABRAS_SCRAPING])
        

        paragraphs = soup.find_all('p')
        if paragraphs:
            text_content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
            if len(text_content.split()) > 100:
                print(f"‚úÖ Fallback p√°rrafos: {len(text_content.split())} palabras")
                return ' '.join(text_content.split()[:MAX_PALABRAS_SCRAPING])
                
    except Exception as e:
        print(f"‚ö†Ô∏è BeautifulSoup fall√≥: {e}")


    try:
        response = requests.get(url, headers=headers, timeout=8)
        response.raise_for_status()
        
        clean = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')
        text = re.sub(clean, ' ', response.text)
        
        text = ' '.join(text.split())
        

        if len(text.split()) > 300: 
            print(f"‚úÖ Regex cleaning (fallback robusto): {len(text.split())} palabras")
            return ' '.join(text.split()[:800]) 
        else:
            print(f"‚ö†Ô∏è Regex cleaning: contenido insuficiente/ruido ({len(text.split())} palabras)")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Regex cleaning fall√≥: {e}")


    print(f"‚ùå Todos los m√©todos fallaron, usando descripci√≥n: {len(fallback_description.split()) if fallback_description else 0} palabras")
    return fallback_description if fallback_description and len(fallback_description.split()) > 30 else None

def validar_contenido_noticia(texto, titulo):
    """Valida que el contenido sea realmente una noticia y no c√≥digo/ruido"""

    if not texto or len(texto.split()) < MIN_PALABRAS_CONTENIDO_VALIDO:
        return False
    

    patrones_no_deseados = [
        r'<!DOCTYPE', r'<html', r'<head>', r'<body>', r'function\s*\(',
        r'var\s+', r'const\s+', r'let\s+', r'classList\.', r'addEventListener',
        r'@media', r'font-family', r'background-color', r'padding:',
        r'margin:', r'display:\s*(flex|block|inline)', r'position:',
        r'z-index:', r'window\.', r'document\.', r'\.getElementById',
        r'\.querySelector', r'\.addClass', r'\.removeClass'
    ]
    
    for patron in patrones_no_deseados:
        if re.search(patron, texto, re.IGNORECASE):
            print(f"‚ö†Ô∏è Contenido rechazado por patr√≥n: {patron}")
            return False
    

    palabras_clave_noticia = ['anunci√≥', 'confirm√≥', 'inform√≥', 'declar√≥', 'seg√∫n', 'fuentes', 
                              'investigaci√≥n', 'estudio', 'datos', 'informe', 'autoridades',
                              'gobierno', 'empresa', 'mercado', 'econom√≠a', 'pol√≠tica']
    
    palabras_encontradas = sum(1 for palabra in palabras_clave_noticia if palabra in texto.lower())
    
    return palabras_encontradas >= 2

def resumir_texto_robusto(texto, titulo):
    """Genera res√∫menes robustos con validaci√≥n de contenido"""
    
    
    if not validar_contenido_noticia(texto, titulo):
        print("‚ùå Contenido no v√°lido para resumir")
        return "Resumen no disponible - contenido insuficiente"
    

    if len(texto.split()) < MIN_PALABRAS_RESUMEN_SIGNIFICATIVO:
        return "Contenido insuficiente para generar un resumen significativo."
    

    if len(texto.split()) > MAX_PALABRAS_SCRAPING:
        texto = ' '.join(texto.split()[:MAX_PALABRAS_SCRAPING])
        print(f"‚úÇÔ∏è Texto recortado para resumen a {MAX_PALABRAS_SCRAPING} palabras.")

    
    prompt = f"""
# CONTEXTO Y ROL
Eres un periodista senior especializado en crear res√∫menes ejecutivos para medios de comunicaci√≥n. Tu tarea es transformar el texto proporcionado en un resumen period√≠stico de alta calidad.

# T√çTULO DE LA NOTICIA: {titulo}

# INSTRUCCIONES ESTRICTAS
## FORMATO:
- EXCLUSIVAMENTE un p√°rrafo continuo
- SIN saltos de l√≠nea, vi√±etas, n√∫meros o encabezados
- LONGITUD: 200-330 palabras (NUNCA exceder {MAX_PALABRAS_RESUMEN} palabras)
- Lenguaje 100% en espa√±ol

## CONTENIDO PERIOD√çSTICO:
Aplica la t√©cnica de las **5W+H** de forma impl√≠cita:
- **QU√â**: Evento principal/descubrimiento/anuncio
- **QUI√âN**: Actores principales (personas, empresas, instituciones)
- **CU√ÅNDO**: Marco temporal relevante
- **D√ìNDE**: Ubicaci√≥n/contexto geogr√°fico
- **POR QU√â**: Causas o motivos subyacentes
- **C√ìMO**: Metodolog√≠a o desarrollo del evento

## ESTILO Y TONO:
- Lenguaje formal pero accesible
- Objetividad absoluta: cero opiniones, cero adjetivos valorativos
- Densidad informativa: m√°xima informaci√≥n en m√≠nimo espacio
- Coherencia temporal: mantener secuencia l√≥gica de eventos
- Eliminar redundancias y informaci√≥n secundaria

## CRITERIOS DE CALIDAD:
‚úÖ PRIORIZAR: Impacto, relevancia, consecuencias
‚úÖ MENCIONAR: Cifras, datos concretos, fechas clave
‚úÖ ESTRUCTURAR: De lo general a lo espec√≠fico
‚ùå ELIMINAR: Lenguaje promocional, sensacionalismo, juicios de valor
‚ùå EVITAR: Repeticiones, informaci√≥n trivial, an√©cdotas irrelevantes

# TEXTO ORIGINAL PARA RESUMIR:
{texto}

# RESULTADO ESPERADO:
[Tu resumen period√≠stico aqu√≠, en un solo p√°rrafo continuo]
"""
    
    try:
        response = model.generate_content(prompt)
        resumen = response.text.strip()
        

        if len(resumen.split()) < 50 or len(resumen.split()) > MAX_PALABRAS_RESUMEN:
            raise ValueError(f"Resumen fuera de los l√≠mites de longitud ({len(resumen.split())} palabras)")
            
        print(f"‚úÖ Resumen generado: {len(resumen.split())} palabras")
        return resumen
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error al generar resumen con Gemini: {repr(e)}")
        
        sentences = re.split(r'[.!?]+', texto)
        meaningful_sentences = [s.strip() for s in sentences if len(s.split()) > 8][:5]
        fallback = ". ".join(meaningful_sentences) + "."
        return fallback if len(fallback) > 50 else "Resumen no disponible por limitaciones t√©cnicas."

def procesar_y_guardar_noticias():
    """Proceso principal robusto de obtenci√≥n y procesamiento de noticias"""
    
    db.inicializar_db()
    
    print("üïí Iniciando proceso de obtenci√≥n de noticias...")
    print("=" * 60)
    print(f"üìä Hora de ejecuci√≥n: {datetime.now()}")
    
    urls_existentes = db.obtener_urls_existentes()
    print(f"üìä Noticias existentes en la base de datos: {len(urls_existentes)}")
   
    todas_las_noticias = []
    categorias_procesadas = 0

    for i, categoria_api in enumerate(CATEGORIAS.keys()):
        print(f"\nüìç Procesando categor√≠a {i+1}/{len(CATEGORIAS)}: {categoria_api}")
        
        try:
            noticias_de_categoria = obtener_noticias_por_categoria(
                categoria_api,
                max_noticias=MAX_NOTICIAS_POR_CATEGORIA,
                urls_existentes=urls_existentes
            )
            
            if noticias_de_categoria:
                todas_las_noticias.extend(noticias_de_categoria)
                categorias_procesadas += 1
                print(f"‚úÖ Categor√≠a {categoria_api}: {len(noticias_de_categoria)} noticias nuevas")
            else:
                print(f"‚ö†Ô∏è Categor√≠a {categoria_api}: 0 noticias nuevas")
                

            if i < len(CATEGORIAS) - 1:
                sleep(2)
                
        except Exception as e:
            print(f"‚ùå Error procesando categor√≠a {categoria_api}: {e}")
            continue
    
    if not todas_las_noticias:
        print("‚ùå No se encontraron noticias NUEVAS v√°lidas para procesar.")
        return {
            "nuevas_guardadas": 0, 
            "mensaje": "No se encontraron noticias nuevas v√°lidas",
            "categorias_procesadas": 0,
            "timestamp": datetime.now().isoformat()
        }
    
    print(f"\nüìù Procesando y guardando {len(todas_las_noticias)} NOTICIAS NUEVAS...\n")
    
    noticias_guardadas = 0
    noticias_fallidas = 0
    
    for art in tqdm(todas_las_noticias, desc="Procesando noticias"):
        try:
            print(f"\nüîç Procesando: {art.get('title')[:60]}...")
            
            texto_completo = scrapear_texto_robusto(
                art.get("url"), 
                art.get("description")
            )
            
            if not texto_completo:
                print(f"‚ö†Ô∏è Sin contenido para: {art.get('title')[:60]}...")
                noticias_fallidas += 1
                continue
                
            print(f"‚úÖ Contenido obtenido: {len(texto_completo.split())} palabras")
            
            resumen = resumir_texto_robusto(texto_completo, art.get("title"))
            
            if not all([art.get("title"), art.get("url"), art.get("publishedAt")]):
                print(f"‚ö†Ô∏è Datos incompletos para: {art.get('title')[:60]}...")
                noticias_fallidas += 1
                continue

            try:
                fecha_obj = datetime.strptime(art.get("publishedAt"), "%Y-%m-%dT%H:%M:%SZ")
                fecha_formateada = fecha_obj.strftime("%Y-%m-%d")
            except (ValueError, TypeError):
                fecha_formateada = datetime.now().strftime("%Y-%m-%d")

            noticia = {
                "titulo": art.get("title").strip(),
                "url": art.get("url"),
                "categoria": art.get("categoria_asignada", "General"),
                "imagen": art.get("image", ""),
                "fuente": art.get("source", {}).get("name", "Desconocida"),
                "fecha": fecha_formateada,
                "resumen": resumen,
                "titulo_hash": generar_hash_titulo(art.get("title"))
            }

            db.insert_noticia(noticia)
            noticias_guardadas += 1
            print(f"‚úÖ Guardada: {art.get('title')[:70]}...")
            
        except Exception as e:
            print(f"‚ùå Error guardando noticia: {e}")
            noticias_fallidas += 1
            continue


    try:
        print("\nüóëÔ∏è ¬†Ejecutando limpieza de noticias antiguas...")
        db.delete_old_noticias(max_months=6)
    except Exception as e:
        print(f"‚ö†Ô∏è Error en limpieza: {e}")
    
    print(f"\nüéØ PROCESO COMPLETADO")
    print("=" * 50)
    print(f"‚úÖ Noticias guardadas: {noticias_guardadas}")
    print(f"‚ùå Noticias fallidas: {noticias_fallidas}")
    print(f"üìä Categor√≠as procesadas: {categorias_procesadas}/{len(CATEGORIAS)}")
    
    stats = db.get_stats()
    print(f"üìà Total en base de datos: {stats['total_noticias']} noticias")
    print(f"üëÜ Total de clics: {stats['total_clics']}")
    print(f"üìÖ Noticias hoy: {stats['noticias_hoy']}")
    
    return {
        "nuevas_guardadas": noticias_guardadas,
        "noticias_fallidas": noticias_fallidas,
        "categorias_procesadas": categorias_procesadas,
        "total_noticias": stats['total_noticias'],
        "total_clics": stats['total_clics'],
        "noticias_hoy": stats['noticias_hoy'],
        "timestamp": datetime.now().isoformat(),
        "proceso_exitoso": True
    }

def ejecutar_crawler():
    """Funci√≥n que ejecuta el crawler y retorna resultados para el endpoint."""
    print("üöÄ INICIANDO CRAWLER DESDE ENDPOINT")
    print("=" * 60)
    
    try:
        resultado = procesar_y_guardar_noticias()
        print("üéØ CRAWLER COMPLETADO EXITOSAMENTE")
        return resultado
    except Exception as e:
        print(f"‚ùå ERROR EN CRAWLER: {e}")
        return {
            "error": str(e), 
            "nuevas_guardadas": 0,
            "noticias_fallidas": 0,
            "categorias_procesadas": 0,
            "timestamp": datetime.now().isoformat(),
            "proceso_exitoso": False
        }

if __name__ == "__main__":
    ejecutar_crawler()