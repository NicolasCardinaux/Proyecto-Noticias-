import os
import requests
from dotenv import load_dotenv
import google.generativeai as genai
from time import sleep
from tqdm import tqdm
from datetime import datetime
import hashlib
import re
import db 
from bs4 import BeautifulSoup
import trafilatura

load_dotenv()
GNEWS_API_KEY = os.getenv("GNEWS_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

ES_PRODUCCION = os.getenv("ENVIRONMENT") == "production"

if not GNEWS_API_KEY or not GEMINI_API_KEY:
    raise ValueError("‚ö†Ô∏è Aseg√∫rate de configurar GNEWS_API_KEY y GEMINI_API_KEY en el archivo .env")

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel("gemini-2.5-flash")

CATEGORIAS = {
    "business": "Negocios", "entertainment": "Entretenimiento", "health": "Salud",
    "science": "Ciencia", "sports": "Deportes", "technology": "Tecnolog√≠a", "general": "General"
}

MAX_NOTICIAS_POR_CATEGORIA = 3  # Aument√© ligeramente para compensar filtros m√°s flexibles
TIEMPO_ESPERA_ENTRE_REQUESTS = 2
MAX_PALABRAS_RESUMEN = 350
MAX_PALABRAS_SCRAPING = 600
MIN_PALABRAS_CONTENIDO_VALIDO = 30  # REDUCIDO de 50 a 30
MIN_PALABRAS_RESUMEN_SIGNIFICATIVO = 40  # REDUCIDO de 80 a 40
MIN_PALABRAS_DESCRIPCION = 40  # REDUCIDO de 80 a 40

RESUMENES_INVALIDOS = [
    "Resumen no disponible - contenido insuficiente",
    "Contenido insuficiente para generar un resumen significativo.",
    "Resumen no disponible por limitaciones t√©cnicas.",
    "Resumen no disponible",
    "contenido insuficiente",
    "limitaciones t√©cnicas"
]

def generar_hash_titulo(titulo):
    return hashlib.md5(titulo.strip().lower().encode('utf-8')).hexdigest()

def limpiar_noticias_existentes_invalidas():
    """Elimina noticias existentes en la base de datos que tengan res√∫menes inv√°lidos"""
    print("üßπ Buscando noticias existentes con res√∫menes inv√°lidos...")
    
    try:
        todas_noticias = db.get_noticias(limit=1000)
        
        noticias_invalidas = []
        
        for noticia in todas_noticias:
            resumen = noticia.get('resumen', '')
            
            if any(invalido.lower() in resumen.lower() for invalido in RESUMENES_INVALIDOS):
                noticias_invalidas.append(noticia['id'])
                print(f"üö´ Encontrada noticia inv√°lida: ID {noticia['id']} - {noticia['titulo'][:50]}...")
        
        if noticias_invalidas:
            print(f"üóëÔ∏è  Eliminando {len(noticias_invalidas)} noticias con res√∫menes inv√°lidos...")
            
            eliminadas_exitosas = 0
            for noticia_id in noticias_invalidas:
                try:
                    if db.eliminar_noticia_por_id(noticia_id):
                        eliminadas_exitosas += 1
                        print(f"‚úÖ Eliminada noticia ID {noticia_id}")
                    else:
                        print(f"‚ö†Ô∏è No se pudo eliminar noticia ID {noticia_id}")
                except Exception as e:
                    print(f"‚ùå Error eliminando noticia ID {noticia_id}: {e}")
            
            print(f"üéØ Limpieza completada: {eliminadas_exitosas}/{len(noticias_invalidas)} noticias inv√°lidas eliminadas")
            return eliminadas_exitosas
        else:
            print("‚úÖ No se encontraron noticias con res√∫menes inv√°lidos")
            return 0
            
    except Exception as e:
        print(f"‚ùå Error en limpieza de noticias existentes: {e}")
        return 0
    
def obtener_noticias_por_categoria(categoria, max_noticias=MAX_NOTICIAS_POR_CATEGORIA, urls_existentes=None):
    if urls_existentes is None:
        urls_existentes = set()
    
    print(f"üì° Buscando {max_noticias} noticias NUEVAS de: '{CATEGORIAS.get(categoria, categoria)}'...")
    
    noticias_nuevas = []
    urls_encontradas = set()
    
    try:
        url = (f"https://gnews.io/api/v4/top-headlines?"
               f"category={categoria}&lang=es&max={max_noticias * 4}&apikey={GNEWS_API_KEY}")  # Aument√© el buffer
        
        sleep(TIEMPO_ESPERA_ENTRE_REQUESTS)
        
        resp = requests.get(url, timeout=15)
        
        if resp.status_code == 429:
            print(f"‚è≥ Rate limit alcanzado para {categoria}, esperando...")
            sleep(15)
            return []
            
        if resp.status_code != 200:
            print(f"‚ùå Error HTTP {resp.status_code} para {categoria}")
            return []
            
        data = resp.json()
        articulos = data.get("articles", [])
       
        for articulo in articulos:
            if not all([articulo.get("url"), articulo.get("title"), articulo.get("description")]):
                continue
                
            url_noticia = articulo.get("url")
            
            descripcion = articulo.get("description", "").strip()
            titulo = articulo.get("title", "").strip()
            
            # CRITERIOS M√ÅS FLEXIBLES - REDUCIDOS
            if (url_noticia not in urls_existentes and 
                url_noticia not in urls_encontradas and
                len(titulo) > 10 and  # REDUCIDO de 15 a 10
                len(descripcion) > MIN_PALABRAS_DESCRIPCION and  # Usando la nueva variable
                not db.noticia_existe(titulo, url_noticia)):
                
                articulo['categoria_asignada'] = CATEGORIAS.get(categoria, "General")
                noticias_nuevas.append(articulo)
                urls_encontradas.add(url_noticia)
                
                if len(noticias_nuevas) >= max_noticias:
                    break
        
    except requests.exceptions.Timeout:
        print(f"‚è∞ Timeout al obtener noticias de {categoria}")
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error de conexi√≥n con GNews para '{categoria}': {e}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error inesperado en {categoria}: {e}")
    
    print(f"‚úÖ Encontradas {len(noticias_nuevas)} noticias v√°lidas para '{CATEGORIAS.get(categoria, categoria)}'")
    return noticias_nuevas

def scrapear_texto_robusto(url, fallback_description=None):
    """Scraping robusto con m√∫ltiples m√©todos de extracci√≥n - CRITERIOS M√ÅS FLEXIBLES"""
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    # Trafilatura - CON UMBRAL M√ÅS BAJO
    try:
        downloaded = trafilatura.fetch_url(url) 
        if downloaded:
            content = trafilatura.extract(
                downloaded, 
                include_comments=False, 
                include_tables=False,
                no_fallback=True
            )
            if content and len(content.split()) > 50:  # REDUCIDO de 100 a 50
                print(f"‚úÖ Trafilatura: {len(content.split())} palabras")
                return ' '.join(content.split()[:MAX_PALABRAS_SCRAPING])
    except Exception as e:
        print(f"‚ö†Ô∏è Trafilatura fall√≥: {e}")

    # BeautifulSoup - CON UMBRALES M√ÅS BAJOS
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        selectores = [
            'article .article-content', 'article .story-content', '.news-content',
            '.entry-content', '.post-content', '[class*="content"]',
            'article p', '.article-body', '.news-body', '.story-text', '.news-text'
        ]
        
        for selector in selectores:
            elements = soup.select(selector)
            if elements:
                text_content = ' '.join([elem.get_text(strip=True) for elem in elements])
                if len(text_content.split()) > 50:  # REDUCIDO de 100 a 50
                    print(f"‚úÖ BeautifulSoup con selector '{selector}': {len(text_content.split())} palabras")
                    return ' '.join(text_content.split()[:MAX_PALABRAS_SCRAPING])
        
        # P√°rrafos individuales - M√ÅS FLEXIBLE
        paragraphs = soup.find_all('p')
        if paragraphs:
            text_content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30])  # REDUCIDO de 50 a 30
            if len(text_content.split()) > 40:  # REDUCIDO de 100 a 40
                print(f"‚úÖ Fallback p√°rrafos: {len(text_content.split())} palabras")
                return ' '.join(text_content.split()[:MAX_PALABRAS_SCRAPING])
                
    except Exception as e:
        print(f"‚ö†Ô∏è BeautifulSoup fall√≥: {e}")

    # Regex cleaning - M√ÅS PERMISIVO
    try:
        response = requests.get(url, headers=headers, timeout=8)
        response.raise_for_status()
        
        clean = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')
        text = re.sub(clean, ' ', response.text)
        
        text = ' '.join(text.split())
        
        if len(text.split()) > 80:  # REDUCIDO de 300 a 80
            print(f"‚úÖ Regex cleaning (fallback robusto): {len(text.split())} palabras")
            return ' '.join(text.split()[:500])  # REDUCIDO el l√≠mite
        else:
            print(f"‚ö†Ô∏è Regex cleaning: contenido insuficiente/ruido ({len(text.split())} palabras)")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Regex cleaning fall√≥: {e}")

    # FALLBACK M√ÅS PERMISIVO
    if fallback_description and len(fallback_description.split()) >= 20:  # REDUCIDO de 30 a 20
        print(f"‚úÖ Usando descripci√≥n fallback: {len(fallback_description.split())} palabras")
        return fallback_description
    
    print(f"‚ùå Todos los m√©todos fallaron, contenido insuficiente")
    return None

def validar_contenido_noticia(texto, titulo):
    """Valida que el contenido sea realmente una noticia - FILTROS MUY FLEXIBLES"""
    
    if not texto or len(texto.split()) < MIN_PALABRAS_CONTENIDO_VALIDO:
        print(f"‚ö†Ô∏è Contenido muy corto: {len(texto.split()) if texto else 0} palabras")
        return False
    
    # PATRONES MUCHO M√ÅS ESPEC√çFICOS para evitar falsos positivos
    patrones_no_deseados = [
        r'<!DOCTYPE', r'<html', r'<head>', r'<body>', 
        r'function\s*\(', r'classList\.', r'addEventListener',
        r'@media', r'font-family', r'background-color',
        r'window\.', r'document\.', r'\.getElementById',
        r'\.querySelector', r'\.addClass', r'\.removeClass'
    ]
    
    # Solo buscar patrones si aparecen m√∫ltiples veces (evitar falsos positivos)
    for patron in patrones_no_deseados:
        matches = re.findall(patron, texto, re.IGNORECASE)
        if len(matches) > 2:  # Solo rechazar si aparece m√°s de 2 veces
            print(f"‚ö†Ô∏è Contenido rechazado por patr√≥n m√∫ltiple: {patron} ({len(matches)} veces)")
            return False
    
    # ELIMINAR patrones que causan falsos positivos con lenguaje natural
    patrones_problematicos = [
        r'var\s+', r'const\s+', r'let\s+',  # DEMASIADOS FALSOS POSITIVOS
        r'padding:', r'margin:', r'display:\s*(flex|block|inline)',
        r'position:', r'z-index:'
    ]
    
    # PALABRAS CLAVE M√ÅS FLEXIBLES
    palabras_clave_noticia = [
        'anunci√≥', 'confirm√≥', 'inform√≥', 'declar√≥', 'seg√∫n', 'fuentes', 
        'investigaci√≥n', 'estudio', 'datos', 'informe', 'autoridades',
        'gobierno', 'empresa', 'mercado', 'econom√≠a', 'pol√≠tica',
        'afirm√≥', 'se√±al√≥', 'explic√≥', 'indic√≥', 'manifest√≥',
        'expres√≥', 'revel√≥', 'destac√≥', 'coment√≥', 'mencion√≥',
        'pa√≠s', 'ciudad', 'presidente', 'ministro', 'director',
        'a√±o', 'mes', 'd√≠a', 'semana', 'horas', 'minutos'
    ]
    
    palabras_encontradas = sum(1 for palabra in palabras_clave_noticia if palabra in texto.lower())
    
    # ACEPTAR incluso si tiene solo 1 palabra clave (para noticias muy cortas)
    tiene_suficientes_palabras_clave = palabras_encontradas >= 1
    
    # Verificar que sea texto legible (no c√≥digo)
    es_texto_legible = (
        len(re.findall(r'[.!?]', texto)) > 2 or  # Tiene puntuaci√≥n
        len(re.findall(r'\b[a-zA-Z√°√©√≠√≥√∫√±]{4,}\b', texto)) > 20  # Palabras largas
    )
    
    return tiene_suficientes_palabras_clave and es_texto_legible

def resumir_texto_robusto(texto, titulo):
    """Genera res√∫menes robustos con validaci√≥n de contenido M√ÅS FLEXIBLE"""
    
    if not validar_contenido_noticia(texto, titulo):
        print("‚ùå Contenido no v√°lido para resumir")
        return "Resumen no disponible - contenido insuficiente"

    if len(texto.split()) < MIN_PALABRAS_RESUMEN_SIGNIFICATIVO:  # Ahora 40 palabras m√≠nimo
        return "Contenido insuficiente para generar un resumen significativo."

    if len(texto.split()) > MAX_PALABRAS_SCRAPING:
        texto = ' '.join(texto.split()[:MAX_PALABRAS_SCRAPING])
        print(f"‚úÇÔ∏è Texto recortado para resumen a {MAX_PALABRAS_SCRAPING} palabras.")

    # PROMPT ADAPTADO PARA TEXTOS M√ÅS CORTOS
    prompt = f"""
# CONTEXTO Y ROL
Eres un periodista senior especializado en crear res√∫menes ejecutivos para medios de comunicaci√≥n. Tu tarea es transformar el texto proporcionado en un resumen period√≠stico de alta calidad.

# T√çTULO DE LA NOTICIA: {titulo}

# INSTRUCCIONES ESTRICTAS
## FORMATO:
- EXCLUSIVAMENTE un p√°rrafo continuo
- SIN saltos de l√≠nea, vi√±etas, n√∫meros o encabezados
- LONGITUD: 100-330 palabras (AJUSTADO para contenido m√°s corto)
- Lenguaje 100% en espa√±ol

## CONTENIDO PERIOD√çSTICO:
Aplica la t√©cnica de las **5W+H** de forma impl√≠cita pero concisa:
- **QU√â**: Evento principal/descubrimiento/anuncio
- **QUI√âN**: Actores principales
- **CONTEXTO**: Informaci√≥n esencial

## ESTILO Y TONO:
- Lenguaje formal pero accesible
- Objetividad absoluta
- Densidad informativa m√°xima
- Adaptado al contenido disponible

## PARA CONTENIDO M√ÅS CORTO:
Si el texto original es breve, crea un resumen conciso pero informativo que expanda ligeramente la informaci√≥n disponible.

# TEXTO ORIGINAL PARA RESUMIR:
{texto}

# RESULTADO ESPERADO:
[Tu resumen period√≠stico aqu√≠, en un solo p√°rrafo continuo, adaptado a la longitud del contenido original]
"""
    
    try:
        response = model.generate_content(prompt)
        resumen = response.text.strip()
        
        # CRITERIOS DE VALIDACI√ìN M√ÅS FLEXIBLES
        if (len(resumen.split()) < 30 or  # REDUCIDO de 50 a 30
            len(resumen.split()) > MAX_PALABRAS_RESUMEN or
            any(invalido in resumen for invalido in RESUMENES_INVALIDOS)):
            raise ValueError(f"Resumen inv√°lido o fuera de l√≠mites ({len(resumen.split())} palabras)")
            
        print(f"‚úÖ Resumen generado: {len(resumen.split())} palabras")
        return resumen
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error al generar resumen con Gemini: {repr(e)}")
        
        # FALLBACK M√ÅS PERMISIVO
        if len(texto.split()) > 30:  # REDUCIDO de 100 a 30
            sentences = re.split(r'[.!?]+', texto)
            meaningful_sentences = [s.strip() for s in sentences if len(s.split()) > 5][:4]  # REDUCIDO umbral
            fallback = ". ".join(meaningful_sentences) + "."
            if len(fallback) > 50:  # REDUCIDO de 80 a 50
                return fallback
        
        return "Resumen no disponible - contenido insuficiente"

def es_resumen_valido(resumen):
    """üî• VALIDA si el resumen es aceptable - CRITERIOS M√ÅS FLEXIBLES"""
    if not resumen or len(resumen.strip()) == 0:
        return False
    
    resumen_lower = resumen.lower()
    if any(invalido.lower() in resumen_lower for invalido in RESUMENES_INVALIDOS):
        return False
    
    if len(resumen.split()) < 25:  # REDUCIDO de 40 a 25 palabras m√≠nimas
        return False
    
    return True

def procesar_y_guardar_noticias():
    """Proceso principal robusto de obtenci√≥n y procesamiento de noticias - M√ÅS PERMISIVO"""
    
    db.inicializar_db()
    
    print("üïí Iniciando proceso de obtenci√≥n de noticias...")
    print("=" * 60)
    print(f"üìä Hora de ejecuci√≥n: {datetime.now()}")
    print(f"üéØ UMBRALES FLEXIBLES: M√≠nimo {MIN_PALABRAS_CONTENIDO_VALIDO} palabras para contenido v√°lido")
    
    noticias_eliminadas = limpiar_noticias_existentes_invalidas()
    
    urls_existentes = db.obtener_urls_existentes()
    print(f"üìä Noticias existentes en la base de datos: {len(urls_existentes)}")
   
    todas_las_noticias = []
    categorias_procesadas = 0

    for i, categoria_api in enumerate(CATEGORIAS.keys()):
        print(f"\nüìç Procesando categor√≠a {i+1}/{len(CATEGORIAS)}: {categoria_api}")
        
        try:
            noticias_de_categoria = obtener_noticias_por_categoria(
                categoria_api,
                max_noticias=MAX_NOTICIAS_POR_CATEGORIA,
                urls_existentes=urls_existentes
            )
            
            if noticias_de_categoria:
                todas_las_noticias.extend(noticias_de_categoria)
                categorias_procesadas += 1
                print(f"‚úÖ Categor√≠a {categoria_api}: {len(noticias_de_categoria)} noticias nuevas")
            else:
                print(f"‚ö†Ô∏è Categor√≠a {categoria_api}: 0 noticias nuevas")
                
            if i < len(CATEGORIAS) - 1:
                sleep(2)
                
        except Exception as e:
            print(f"‚ùå Error procesando categor√≠a {categoria_api}: {e}")
            continue
    
    if not todas_las_noticias:
        print("‚ùå No se encontraron noticias NUEVAS v√°lidas para procesar.")
        return {
            "nuevas_guardadas": 0, 
            "existentes_eliminadas": noticias_eliminadas, 
            "mensaje": "No se encontraron noticias nuevas v√°lidas",
            "categorias_procesadas": 0,
            "timestamp": datetime.now().isoformat()
        }
    
    print(f"\nüìù Procesando y guardando {len(todas_las_noticias)} NOTICIAS NUEVAS...\n")
    
    noticias_guardadas = 0
    noticias_fallidas = 0
    noticias_rechazadas = 0  
    
    for art in tqdm(todas_las_noticias, desc="Procesando noticias"):
        try:
            print(f"\nüîç Procesando: {art.get('title')[:60]}...")
            
            texto_completo = scrapear_texto_robusto(
                art.get("url"), 
                art.get("description")
            )
            
            if not texto_completo:
                print(f"‚ö†Ô∏è Sin contenido para: {art.get('title')[:60]}...")
                noticias_fallidas += 1
                continue
                
            print(f"‚úÖ Contenido obtenido: {len(texto_completo.split())} palabras")
            
            resumen = resumir_texto_robusto(texto_completo, art.get("title"))
            
            if not es_resumen_valido(resumen):
                print(f"üö´ RESUMEN INV√ÅLIDO - Rechazando noticia: {resumen[:50]}...")
                noticias_rechazadas += 1
                continue
            
            if not all([art.get("title"), art.get("url"), art.get("publishedAt")]):
                print(f"‚ö†Ô∏è Datos incompletos para: {art.get('title')[:60]}...")
                noticias_fallidas += 1
                continue

            try:
                fecha_obj = datetime.strptime(art.get("publishedAt"), "%Y-%m-%dT%H:%M:%SZ")
                fecha_formateada = fecha_obj.strftime("%Y-%m-%d")
            except (ValueError, TypeError):
                fecha_formateada = datetime.now().strftime("%Y-%m-%d")

            noticia = {
                "titulo": art.get("title").strip(),
                "url": art.get("url"),
                "categoria": art.get("categoria_asignada", "General"),
                "imagen": art.get("image", ""),
                "fuente": art.get("source", {}).get("name", "Desconocida"),
                "fecha": fecha_formateada,
                "resumen": resumen,
                "titulo_hash": generar_hash_titulo(art.get("title"))
            }

            db.insert_noticia(noticia)
            noticias_guardadas += 1
            print(f"‚úÖ Guardada: {art.get('title')[:70]}...")
            
        except Exception as e:
            print(f"‚ùå Error guardando noticia: {e}")
            noticias_fallidas += 1
            continue

    try:
        print("\nüóëÔ∏è  Ejecutando limpieza de noticias antiguas...")
        db.delete_old_noticias(max_months=6)
    except Exception as e:
        print(f"‚ö†Ô∏è Error en limpieza: {e}")
    
    print(f"\nüéØ PROCESO COMPLETADO")
    print("=" * 50)
    print(f"‚úÖ Noticias guardadas: {noticias_guardadas}")
    print(f"üßπ Noticias existentes eliminadas: {noticias_eliminadas}")
    print(f"üö´ Noticias rechazadas (resumen inv√°lido): {noticias_rechazadas}")
    print(f"‚ùå Noticias fallidas: {noticias_fallidas}")
    print(f"üìä Categor√≠as procesadas: {categorias_procesadas}/{len(CATEGORIAS)}")
    
    stats = db.get_stats()
    print(f"üìà Total en base de datos: {stats['total_noticias']} noticias")
    print(f"üëÜ Total de clics: {stats['total_clics']}")
    print(f"üìÖ Noticias hoy: {stats['noticias_hoy']}")
    
    return {
        "nuevas_guardadas": noticias_guardadas,
        "existentes_eliminadas": noticias_eliminadas,
        "noticias_rechazadas": noticias_rechazadas,
        "noticias_fallidas": noticias_fallidas,
        "categorias_procesadas": categorias_procesadas,
        "total_noticias": stats['total_noticias'],
        "total_clics": stats['total_clics'],
        "noticias_hoy": stats['noticias_hoy'],
        "timestamp": datetime.now().isoformat(),
        "proceso_exitoso": True
    }

def ejecutar_crawler():
    """Funci√≥n que ejecuta el crawler y retorna resultados para el endpoint."""
    print("üöÄ INICIANDO CRAWLER DESDE ENDPOINT")
    print("=" * 60)
    
    try:
        resultado = procesar_y_guardar_noticias()
        print("üéØ CRAWLER COMPLETADO EXITOSAMENTE")
        return resultado
    except Exception as e:
        print(f"‚ùå ERROR EN CRAWLER: {e}")
        return {
            "error": str(e), 
            "nuevas_guardadas": 0,
            "existentes_eliminadas": 0,
            "noticias_rechazadas": 0,
            "noticias_fallidas": 0,
            "categorias_procesadas": 0,
            "timestamp": datetime.now().isoformat(),
            "proceso_exitoso": False
        }

if __name__ == "__main__":
    ejecutar_crawler()